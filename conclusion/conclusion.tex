\section{Summary}
The theme of this thesis is data integration. We started with outlining an empirical technique to calculate functional similarity of datasets using the concept of cluster similarity. This could be used as an index of microarray dataset similarity. We have also showed that similarity values gradually fall with increasing fraction of dissimilar data. We have established that as more diverse data-sets are merged then the similarity to individual data-sets (which have more local patterns) is reduced and the dominant ones overshadow the weaker signals. So, before taking a blind integrative approach, much care should be taken to ensure that we mix only similar types of data. We should also be careful about the choice of normalization method. In our results we demonstrated that normalization can distort the data and affect the resulting clusters significantly.

In order to integrate different types of datasets, we have proposed a technique to integrate two different types of  datasets where one is acting as a source of supervision on the clustering of the other. The source of supervision is in the form of binary constraints derived from DNA-binding, PPI and TF-gene interactions data, and are applied on microarray data. By replicating the trend available in the DNA-binding data, our results independently demonstrated that the information available in it has been successfully incorporated in the combined dataset. We computed the biological significance of the combined datasets using Gene Ontology annotations. 

Finally, we have proposed another technique to integrate two diverse datasets where one of the datasets is non-vectorial. For this, we have used the principle of maximum entropy considering it as the most valid approach under the unsupervised clustering setting where we have no other evidence regarding the weights to be assigned to individual datasets. Again, we computed the biological significance of the combined datasets using Gene Ontology annotations. 

\section{Challenges and Future Directions}
\begin{description}
\item[Holistic Data Integration] Transcriptional regulation occurs at multiple points from transcription to actual protein synthesis. It is well known that transcription activity (\ac{mRNA} concentration) alone is not a perfect indicator of protein concentration \citep{Griffin02Complementary} as there are many post translational factors, e.g. \ac{mRNA} stability, protein degradation, post-translational modifications, that affect the process. As more protein concentration data (\textit{proteome}) and newer types of data, e.g. nucleosome positions \citep{segal2007genomiccode,field2008distinct,kaplan2008nucleosome} become available, we need to develop techniques that can integrate all these and future sources of data in order to develop more precise models of regulation. Some of the ways in which our work could be extended are detailed below.
\begin{itemize}
    \item For semi-supervised clustering, our work can be extended by using other sources as prior knowledge, for example the constraints derived from known genetic interactions based on metabolic interaction data or pathway data. In future, when different types of data from experiments conducted simultaneously become available, the reliability of our technique would increase.

    \item Other sources of gene similarity could be derived from known genetic interactions based on metabolic interaction data or pathway data or similarity of promoter gene sequences \citep{jean2006kernels} and then used with the model discussed in Chapter-5.

    \item We have used the maximum entropy technique in order to combine two similarity matrices and used this integration for clustering which is known as an unsupervised classification technique in machine learning literature. We would also like to explore the possibilities of this integrated matrix which has all the properties of a \textit{kernel} to do SVM based classification and compare the results to other methods of kernel combination \citep{lanck04kerneldatafusion} and shrinkage based methods.

    \item Many researchers have used microarray data for cancer classification. Covariance matrix estimation is essential for many of these classification techniques, e.g. \ac{LDA} and \ac{RDA} as they involve computing the inverse of the covariance matrix. If the dataset has a large number of variables but only few samples, it is known as the ``Small $n$, Large $p$'' or $n\ll p$ problem. Microarray datasets are a typical case of this because the number of genes (p) is very large while the number of available microarray samples (n) is very limited. In such a case, the estimated covariance matrix looses its full rank (rank deficient). This leads to many unwanted properties. If the covariance matrix is not full rank then it is not positive definite anymore, which is a requirement for many algorithms that might use this covariance matrix as a similarity matrix, e.g. kernel based classifiers (SVM). Another bigger problem is that this rank deficient covariance matrix is not 
invertible. There has been a lot of research in proposing better estimators of the covariance matrix, e.g. \citet{schaefer05shrinkage}. As shown by \citet{carlos05maximum} where this formulation was first used for face detection, the maximum entropy principle is an ideal candidate for an estimator of the covariance matrix when some prior knowledge is available. We intend to use it for cancer classification and compare the results with existing shrinkage based methods e.g., \citet{Tai2008Incorporating}, who combined covariance matrices and then used the resulting covariance matrix in \ac{RDA} for cancer classification.  
\end{itemize}

\item[System Dynamics] Another growing area of research is based on more detailed modelling using reaction kinematics of gene products. This could help in understanding not only the qualitative models of regulation but also detailed quantitative ones. 

\item[Prior Knowledge] Most of the past research in molecular biology involved working with a small number of genes. This has led to the accumulation of a huge amount of biological knowledge. Genome wide global modelling of regulation has mostly used this high quality data for validation of the results. Apart from validation, this prior biological knowledge could be used to produce better models by integrating them along with other sources of experimental data.

\item[Incoherently Integrated Datasets] Orphanides and Reinberg (2002) argue very explicitly that there is no single model of regulation and each cell process has evolved its own detailed regulation model. Moreover, we usually observe only a few snapshots of these processes, which makes it very hard to reconstruct the underlying mechanisms. The data that is integrated comes from various laboratories where experiments are done under different conditions and with different platforms. We must be very careful while integrating such data and care must be taken to check beforehand if the data shows similar trends. The above conditions are some of the reasons why some researchers \citep{dolinski2005changing} have found the amount of overlap in the results based on different datasets to be small. 
 
\item[We Don't Know Biology fully!] Another big challenge that inhibits precise modelling of the process is lack of available data about the 3D structure of chromatin (DNA). Apart from the promoter sequence, the 3-D structure of chromatin decides whether a transcription factor is allowed access to a certain position or not. Sometimes a transcription factor itself facilitates changes in the chromatin structure that allows it access to the promoter sequence. Better techniques of modelling the chromatin structure will definitely aid in a better regulation model.

\item[Complexities of Higher Organisms] Simple unicellular organisms have the advantage that the sample of cells used in an experiment is homogeneous. Each cell is assumed to be performing the same regulatory actions. Based on the results so far, we are far away from a fully comprehensive model of regulation in even simpler organisms like yeast. Higher organisms pose other challenges because of cell and tissue heterogeneity. Apart from this, multi-cellular organisms are a big challenge as it's very difficult to segregate the expression of one cell from its neighbouring ones. Most genomic techniques measure an average signal in a sample from a cell population. When analysing a heterogeneous tissue, this is a big concern as individual signals from different cell types are obfuscated. Moreover, the averaging effect introduces an additional source of noise as the proportions of different cells are different across samples. 

\item[Validation of Results] Interpretation of results is very hard because it is done indirectly. Because of the huge quantity of hypothesis that could be derived from the clustering results, it is not possible to validate them experimentally. So most of the validation is done indirectly using other sources of data, e.g. gene ontology annotations. Even though gene ontology databases have contributed significantly to the creation of a common language to describe properties, we do not have annotations for all genes and gene products. Without high quality annotations, the best algorithms are rendered useless as we can never know how accurate they are. Another issue is that no standardised data sets exist on which existing and future techniques could be compared. Recent years have seen a huge increase in research on innovative techniques. Yet, there are no gold standards in validation unlike standardised data sets in other fields like Information Retrieval. We need more standard datasets and better validation metrics to more fruitfully analyse the effectiveness of various algorithms and measure their effectiveness progressively.
 
\end{description}

\section{Final Remarks} 

Despite all the challenges, high-throughput technologies have changed the research focus from studying a handful of genes to studying interactions at the whole genome level. The explosion in data generation has made Biology quickly move towards becoming an information science. Data integration seems to be the only approach which can help us understand the complex underlying processes responsible for functioning of organisms. Extensive amount of further research is required both in the measurement and analysis processes to improve our understanding of how genes interact. We have only begun to understand regulation quantitatively and have a long way to go before we can construct fully detailed regulatory network models. 

Future research in the area of integration will continue as more data of different types become available. The focus will likely shift towards integration of data from multiple cell types, conditions and even organisms. Apart from integration techniques, future research is likely to move towards better validation of the various techniques and the creation of gold standards against which results can be assessed. 

Despite all the challenges, positive results have been achieved with human tumour expression data while studying both individual cancers and, with an integrative approach \citep{segal04module}, simultaneously studying a large cancer compendium of multiple datasets. These have shown that the future of this area of research is bright.