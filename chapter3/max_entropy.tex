% Add a section on Implementation Issues, choice of libraries, language etc
% compare normalized vs non-normalized combination of kernels
% maybe compare our own validation technique, p-value graph, pvalue avg
\begin{quote} ``But since the affairs of men rest still incertain, Let's reason with the worst that may befall.'' - \textit{William Shakespeare (Julius Caesar)}\end{quote} 
\section{Introduction}
As we saw in the previous chapters, each of the current datasets, e.g. microarrays, DNA-binding, protein-protein interaction and sequence datasets, provide a partial 
and noisy picture of cell regulation. Hence, integration among these is required in order to obtain an improved picture of the underlying process. Initial methods 
of data integration in regulatory module discovery were mostly ad-hoc approaches that used clustering with some form of prior knowledge. Later on these were 
enhanced to incorporate model based clustering methods as well. One of the major drawbacks of these techniques was that they worked well on vectorial data but as 
soon as other types of data were encountered, the principled nature of the algorithms broke down and they had to resort to ad-hoc statistical techniques for finding 
correlations in datasets.

In the previous chapter we proposed a similarity based method which is very pertinent for non-vectorial data as there are established techniques to compute similarity 
from these. We will continue using similarity based techniques in this chapter. The bigger challenge that we saw in the last chapter was that of 
\textit{ad hoc} combination of datasets. Since they are reported as p-values, the DNA-binding data could be interpreted as similarity values. 
The similarity values in the DNA-binding dataset were converted into constraints and then combined to the microarray data using \textit{ad hoc} p-value thresholds. 
In order to do the integration in a \textit{principled} manner, we needed a framework under which various types of data could be integrated and their effects analyzed. 
Various earlier researchers have used the Bayesian framework for merging data, but in our opinion it is unsuitable to cope with non-vectorial data (strings, graphs) 
in a principled manner as it was primarily developed for vectorial data. 

To summarise, the problem now is reduced to having two similarity matrices and we need some method to integrate them. A simpler approach to integrating matrices 
is the \textit{shrinkage} method. When there are two similarity matrices $K_{1} and K_{2}$, a final combination $K$ could be written as,
\begin{eqnarray}
K &=& \mu K_{1}+(1-\mu)K_{2}
\end{eqnarray}
which represents a \textit{convex combination}\footnote{A convex combination is a linear combination of where all coefficients are non-negative and sum up to 1} of $K_{1}$ and $K_{2}$ with the shrinkage parameter $\mu$ ranging between 0 and 1, and controlling what fraction of each similarity matrix contributes towards the final matrix. The \textit{shrinkage} method is named so because depending on the $\mu$, we shrink the contribution of the original evidence. For example, if we are combining microarray data with \ac{PPI} data to improve the predictions based just on microarray data, then the contribution of microarray data is being shrunk from its original contribution (which is 1). Optimum $\mu$ values can be chosen after running these various weight combinations of datasets through the Spectral clustering algorithm and then optimizing for the best cluster quality using the Dunn or Davies-Bouldin index values. While this is reasonable from a practical viewpoint, its not very principled. That motivates us towards our next step which is to use the principle of maximum entropy (Section-\ref{kern_integration}) in order to merge the similarity matrices. As we will see in following sections, this allows us to merge two datasets when there is no evidence available regarding their individual importance. For example, when we have two noisy data sources e.g. microarray and \ac{PPI}, and no other evidence regarding their individual importance, we can combine them to get better inference using the principle of maximum entropy.

As discussed in Section-\ref{kern_integration}, we need a more specialised version of the similarity matrix for maximum entropy integration. The extra requirement is that 
our similarity matrices should be positive semi-definite. There is a separate but similar branch of machine learning that operates only on such matrices and are 
known as \textit{kernel methods}. We describe them in Section-\ref{kern_methods}. Throughout this chapter, we have used similarity matrices, kernels and kernel 
matrices interchangeably to refer to \textit{positive semi-definite symmetric similarity matrices}.

\subsection{Spectral or eigen-decomposition}
Spectral or eigen decomposition of a symmetric $n \times n$ matrix $\mathbf{A}$ is represented as
\begin{displaymath}
    \mathbf{A} = \mathbf{Q} \Lambda \mathbf{Q}^{-1}  
\end{displaymath}
where $\Lambda = diag(\lambda_{i})$ is the diagonal matrix, $\lambda_{i}$'s are the eigenvalues of $\mathbf{A}$ and $\mathbf{Q}=[q_{1},\dots,q_{n}]$ is the square $n \times n$ matrix whose $i_{th}$ column is the basis eigenvector $q_{i}$ of $\mathbf{A}$.
  
A symmetric matrix $\mathbf{A}$ is said to be \textit{positive definite} if
\begin{displaymath}
    \mathbf{x}^{T}\mathbf{A}\mathbf{x} > 0 \mbox{ for all non-zero x}  
\end{displaymath}

A symmetric matrix $\mathbf{A}$ is said to be \textit{positive semi-definite} if
\begin{displaymath}
    \mathbf{x}^{T}\mathbf{A}\mathbf{x} \geq 0 \mbox{ for all non-zero x}  
\end{displaymath}
All the eigenvalues of a positive definite matrix are positive whereas the eigenvalues of a positive semi-definite matrix are non-negative.  

\section{Kernel Methods} \label{kern_methods}

\textit{Kernel methods} are algorithms that operate on a type of data representation known as a \textit{kernel} matrix. Kernel matrices provide a general framework to represent data and satisfy certain mathematical properties. A kernel matrix is defined not in terms of individual variables but in terms of pairwise similarity among all variables. So, instead of using a mapping $\phi:\mathcal{X}\rightarrow\mathcal{F}$ to represent each object $\mathbf{x}\in\mathcal{X}$ by $\phi(\mathbf{x})\in \mathcal{F}$, a real valued similarity function $k:\mathcal{X} \times\mathcal{X}\rightarrow\mathbb{R}$  is used and the dataset with n variables is represented by a n $\times$ n matrix of pairwise similarities $k_{ij}=k(\mathbf{x}_{i},\mathbf{x}_{j})$. The most significant fact regarding these methods is that once we have a kernel matrix representation of the data then the original data is not required and the methods can work on just these matrices. This is where the real beauty of these methods arise as different types of data types do not necessitate changes in the underlying algorithm. Kernel methods require that a kernel matrix is \textit{symmetric} and \textit{positive semi-definite}. This means that if $k$ is an $n \times n$ matrix of pairwise similarities then $k_{i,j}=k_{j,i}$ for $1 \leq i,j \leq n$, and $\mathbf{c}^\top k \mathbf{c} \geq 0$ for any $c \in \mathbb{R}^{n}$. This also implies that the matrix has non-negative real eigenvalues. 

Each similarity value ($k_{i,j}$) in a kernel matrix is calculated using a so called kernel function ( k(x,y) ) that acts a \textit{suitable} similarity between the variables. Hence, a real valued kernel matrix could be obtained for diverse data types (strings and graphs) as long as a similarity function can be defined over a pair. This nice property leads to complete separation of similarity function definition from the algorithms that operate on these matrices. This is specially useful in bioinformatics because of diverse types of datasets (as pointed in previous chapter) where a real valued representation of individual variables is non intuitive while a similarity score makes sense, e.g. genomic sequences. We will see different types of kernels in Section-\ref{kern_types}. 

\subsection{Various kernel or similarity functions}\label{kern_types}
We provide a short description of various possible kernels for different data types (vectors, strings and graphs) and their properties.

\subsubsection{Vector Data}
\begin{itemize}
 \item The \textit{Linear} or \textit{Dot kernel} is the simplest one.
\begin{equation}
 k_{L}(\textbf{x},\textbf{x}^{'})=\textbf{x}^{T}\textbf{x}^{'}
\end{equation} 
 \item The \textit{Polynomial kernel} is a more general case of the linear kernel
\begin{equation}
 k_{Poly}(\textbf{x},\textbf{x}^{'})=(\textbf{x}^{T}\textbf{x}^{'}+c)^d
\end{equation} 
where d is the degree of the polynomial and c is a constant. When c is non-zero then this kernel corresponds to a feature space spanned by all products of at most 2 variables i.e., ${\lbrace 1,x_{1},x_{2},x_{1}^{2},x_{1}x_{2},x_{2}^{2} \rbrace}$. When c is zero then this space is restricted to only the products of exactly 2 variables i.e., ${\lbrace x_{1}^{2},x_{1}x_{2},x_{2}^{2} \rbrace}$.
 \item The most popular and widely used kernel function used for real data is the \textit{Gaussian} or \textit{Radial Basis Function (RBF) kernel}
\begin{equation}
 k_{G}(\textbf{x},\textbf{x}^{'})=exp \left( -\frac{{\parallel \textbf{x}-\textbf{x}^{'} \parallel}^{2}}{2\sigma^{2}}\right)
\end{equation}
the width of the Gaussian being controlled using $\sigma$. This affinity function naturally encodes the local neighbourhood property and its value falls rapidly as the pairwise dissimilarity increases.
\item Another popularly used kernel is the \textit{Sigmoid kernel}

\begin{equation}
 k_{S}(\textbf{x},\textbf{x}^{'})=(k\textbf{x}^{T}\textbf{x}^{'}+\theta) 
\end{equation}
where $k>0$ and $\theta < 0$ are the \textit{gain} and \textit{threshold}. 

\end{itemize}

\subsubsection{Graph data}
A graph is informally defined as a set of \textit{nodes} connected by \textit{edges}. In bioinformatics, typical examples of a graph would be the interactions between the proteins of an organism or the interaction network representing the metabolic pathway. Other common examples of such graphs are social networks and hyperlinked internet web pages. While a graph represents \textit{local similarity} i.e., a node's direct interactions in its neighbourhood, we need a similarity function that represents \textit{global similarity} i.e., a node's interaction to every other node in the graph. The simplest measure of similarity on a graph is the shortest-path distance, but it is not positive semi-definite which is our requirement. Apart from this, this is very sensitive to insertions and deletions of edges. A more robust similarity measure is required which could perhaps average over many paths. The physical process of diffusion suggests a natural way of propagating such local information and has led to the most popular type of similarity on graphs known as the \textit{diffusion} kernel \citep{Kondor02diffusion}.

Laplacian $L$ of an \textit{undirected unweighted} graph is defined as,
\begin{eqnarray}
L_{i,j} &=& \begin{cases}
             -1 & \text{for i $\sim$ j,} \\
	     d_{i} & \text{for i=j,} \\
	     0 & \text{otherwise}
            \end{cases}
\end{eqnarray}
where $i \sim j$ implies that $i$ and $j$ are connected by an edge and $d_{i}$ is the number of edges originating from $i_{th}$ node. The kernel function on the graph can be defined using the negative of this Laplacian ($H=-L$) as
\begin{eqnarray}
K_{\beta} = e^{\beta H}= \lim_{m->\infty}\big ( \mathbf{I}+\frac{\beta H}{m}\big )^m
\end{eqnarray}
where $\beta$ is a positive constant and $\mathbf{I}$ is an identity matrix. $K_{\beta}$ represents an exponential family of similarity functions with generator $H$ and bandwidth parameter $\beta$. Using power series expansion this can be expanded to
\begin{eqnarray}
K_{\beta}=  \mathbf{I}+ \beta H +\frac{\beta ^2 H^2}{2} +\frac{\beta ^3 H^3}{3!} + \dots \label{diffusion}
\end{eqnarray}

Note that $e^{\beta H}$ yields a matrix but it is not the same as component-wise exponentiation $e^{\beta H_{ij}}$. If a matrix is diagonal then its exponential can be obtained by just exponentiating every entry on the diagonal, i.e., $e^{D}=diag(e^{d_{11}},e^{d_{22}},\dots,e^{d_{nn}})$. This is an important property that could be used for computing the exponential. If we diagonalise $H$ i.e., if $H = UDU^{-1}$ and D is diagonal, then $e^{H} = Ue^{D}U^{-1}$. Based on this, we have used the technique discussed in \citet{Moler2003Nineteen} to compute our matrix exponentials. It involves computing the normalized eigenvalues and eigenvectors of $H$.
\begin{equation}
    H = \sum_{i=1}^{n}v_{i}\lambda_{i}v_{i}^{T}
\end{equation}
which when replaced in Equation-\ref{diffusion}
\begin{equation}
   K_{\beta}=\sum_{i=1}^{n}v_{i}e^{\beta \lambda_{i}}v_{i}^{T}
\end{equation}

This similarity function is also known as the \textit{diffusion} function because its differential equation form resembles the diffusion equation of heat through continuous media in classical physics \citep{Kondor02diffusion}. The function of $\beta$ is to control the extent of diffusion similar to the $\sigma$ of the Gaussian kernel. In fact, as shown in \citet{kernel_methodsvert2004}, there is straightforward correspondence between the diffusion kernel and the Gaussian kernel. The former can be considered a discretized version of the latter. In the next section we discuss our actual technique of similarity matrix integration. 

\subsection{From similarities to a valid kernel}
Sometimes we have a well defined measure of similarity between a pair of objects, but the resulting matrix is not a valid kernel matrix according to the strict definition of positive semi-definiteness. In such cases, two methods have been proposed in the literature that may be used to convert the similarity matrix to a valid kernel. \citet{tsuda99supportasymmetric} have proposed a principled technique called \textit{empirical kernel map}. \citet{roth2002going} have proposed an ad-hoc technique of eigen-decomposition of the similarity matrix and then removal of negative eigenvalues. They have also showed that this preserves the cluster structure of the data. When we are not sure if the similarity matrix that we have obtained is a kernel matrix then one of these techniques could be used to make it a kernel matrix. 
\subsection{Kernel normalization}
In order to add kernels, we need to normalize them so that they are on the same scale. Given an unnormalized kernel matrix, K, the normalized version is
\begin{equation}
    \hat{K_{ij}} = \frac{K_{ij}}{\sqrt{K_{ii} \times K_{jj}}}
\end{equation} 
This can be easily computed if we define $A = (1/\sqrt{K_{11}}, \dots, 1/\sqrt{K_{nn}})$. Then, $\hat{K} = K \ast (AA^{T})$, where $\ast$ denotes element-wise product.
 
\section{Principle of Maximum Entropy}
\subsection{Entropy} \label{information_theory}
While the term \textit{entropy} is popularly associated with thermodynamics, the entropy which we describe here comes from \textit{information theory}. This branch of applied mathematics and electrical engineering which deals with quantification of information was introduced by Claude E. Shannon in his seminal paper \citep{sha48mathematical}. While this original paper dealt with the engineering problem of the transmission of information over a noisy channel, the scope of information theory has widened a lot and touches subjects as diverse as cryptography to neurobiology. The most fundamental result of this theory is the \textit{source coding theorem}, according to which, on average, the number of bits needed to represent the result of an uncertain event is given by its \textit{entropy}. In other words, \textit{entropy} is a measure of the uncertainty associated with a random variable. 

If a discrete random variable $X$ takes values $x_{1}, \dots, x_{n}$ then its entropy $H$ is
\begin{displaymath}
    H(X) = E(I(X))
\end{displaymath}

where E is the expected value and I(X) is the \textit{information} content or \textit{self-information} of X. Now, if $p(x_i)$ is the probability of $X$ taking value $x_{i}$ then the entropy can explicitly be written as
\begin{displaymath}
    H(X) = \sum_{i=1}^n p(x_i)I(x_i) = -\sum_{i=1}^n p(x_i) \log_b p(x_i),
\end{displaymath}

where b denotes the base of the logarithm. The unit of entropy is the \textit{bit} or \textit{nat} for bases 2 and e respectively. If any of the probabilities vanish ($p(x_i) = 0$ for any $i$, we use the fact that $\lim_{p\to0}p\log p = 0$ and hence the value for that particular $i$ is zero. 

\textit{Differential entropy} also known as continuous entropy tries to extend the idea of Shannon entropy which is restricted to random variables taking discrete values to continuous probability distributions, e.g. Gaussian distribution. Another widely used measure of entropy for the continuous case is the \textit{relative entropy} of a distribution also popularly known as the KL divergence (refer Section-\ref{kl-divergence}). We will later maximize the differential entropy associated with a 
Gaussian distribution in order to merge similarity matrices (refer Section-\ref{kern_integration}). 


\subsection{Principle of maximum entropy} \label{maxent_principle}
Before we discuss our technique in detail, here we discuss the background and philosophical underpinnings of principle of maximum entropy. While in the earlier section we made a strict distinction between entropy associated to thermodynamics and information theory, at a more philosophical level, connections can be made between these two seemingly unrelated subjects. According to E.T. Jaynes in his seminal papers \citep{jaynes57maxent, jaynes82onrationale} 
\begin{quotation}
Thermodynamics should be seen as an application of information theory and the thermodynamic entropy is interpreted as being an estimate of the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics. 
\end{quotation}
He proposed correspondence between statistical mechanics and information theory and suggested that the entropy in statistical mechanics, and the information entropy in information theory, are essentially the same thing. Consequently, statistical mechanics should be seen just as a particular application of a general tool of logical inference and information theory.

Suppose some testable information about a probability distribution is known. If we consider the set of all probability distributions which encode this information then the \textit{principle of maximum entropy (MaxEnt)} states that the probability distribution which maximizes the information entropy in view of the testable information is the true probability distribution. By choosing to use the distribution with the maximum entropy allowed by our information, we are choosing the most \textit{uninformative} distribution possible. If we choose any distribution with lower entropy then that would imply that we are assuming information which we do not have. On the other hand, if we choose a distribution with a higher entropy that would violate the constraints of the information we possess. Thus the maximum entropy distribution is the only reasonable distribution. Maximum entropy principles are used to choose the smoothest distributions out of all possible distributions. 

In our context, intuitively, each similarity matrix represents a distribution and we need to merge them so that the final distribution doesn't make assumptions about the individual weights of the matrices because that information is unavailable. We allow maximum entropy for the resulting distribution implying no assumptions whatsoever. This is the only approach available to us for kernel integration in the unsupervised domain.

Information theory as shown in Section-\ref{information_theory} defines \textit{information} in terms of probability distributions thus providing us with a quantitative measure of uncertainty (entropy) or ignorance. This can be maximized to find the maximally unbiased probability distribution. 

\section{Maximum Entropy Kernel Integration} \label{kern_integration}

We assume the \textit{similarity matrix} which is a symmetric positive semi-definite matrix to be the \textit{covariance matrix} of a \textit{Gaussian} distribution. Based on the earlier justification for the maximum entropy principle, we need to combine two similarity matrices such that the resulting one has maximum entropy. 

Now, a Gaussian distribution is represented as,
\begin{equation}
p(\mathbf{x|\mu,\Sigma})=\frac{1}{(2\pi)^{n/2}\lvert\Sigma\rvert^{1/2}}\exp \big\lbrace -\frac{1}{2}(\mathbf{x-\mu})^{T}\Sigma^{-1} (\mathbf{x-\mu}) \big \rbrace
\end{equation}
where $\lvert\Sigma\rvert$ is the determinant of the covariance matrix $\Sigma$, and $\mu$ is the mean of distribution. Its differential entropy is given by \citep{brookes2005matrix},
\begin{eqnarray}
H(p(\mathbf{x})) &=& -\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \dots \int_{-\infty}^{+\infty}p(\mathbf{x})ln (p(\mathbf{x})) d\mathbf{x} \\
&=& \frac{1}{2}(n+n ln(2\pi)+ln|\Sigma|) \label{diff_entropy_gauss}
\end{eqnarray}
In order to maximize $H(p(\mathbf{x}))$ we can ignore the first two terms ($n$ and $n ln(2\pi)$) in maximizing equation-\ref{diff_entropy_gauss} as they are constants. So, it becomes a problem of maximizing the $ln|\Sigma|$ term. 
We know that the determinant of a symmetric matrix is equal to the the product of its eigenvalues, i.e.,
\[
|\Sigma|=\prod_{i=1}^{k}\lambda_{i}
\]
where $\lambda_{i}$ ($i=1\dots k$) are the $k$ eigenvalues of $\Sigma$. Therefore,
\begin{eqnarray}
ln|\Sigma|&=& ln\big (\prod_{i=1}^{k}\lambda_{i} \big) \\
&=& \sum_{i=1}^{k} ln (\lambda_{i}) \label{max_det}
\end{eqnarray}
Also, since logarithmic functions are monotonically increasing, so we can restate that \textit{in order to maximize the entropy of a Gaussian distribution, we need to maximize the $ln|\Sigma|$\footnote{this is also popularly known as log det maximization in optimization theory literature \citep{boyd2004convexopt}} which is equivalent to maximizing the sum of the eigenvalues of its covariance matrix}. Now assume that our covariance matrix is a combination of two covariance matrices and so can be rewritten as
\begin{eqnarray}
K &=& \sum_{i=1}^{2}\mu_{i}K_{i} \\
&=& \mu_{1}K_{1}+\mu_{2}K_{2}
\end{eqnarray}
where $\mu_{1}+\mu_{2}=1$. Now, according to spectral decomposition of a symmetric matrix,
\begin{eqnarray}
\Lambda &=& UKU^{T} \\ 
&=& U\big (\mu_{1}K_{1}+\mu_{2}K_{2} \big )U^{T} \\
&=& \mu_{1}UK_{1}U^{T}+\mu_{2}UK_{2}U^{T}  \\
&=& \mu_{1}Z_{1}+ \mu_{1}Z_{2} \label{eigv_split_1} 
\end{eqnarray}
where U is orthonormal and $\Lambda=diag[\lambda_{1},\lambda_{2},\dots,\lambda_{n}]$ is the diagonal matrix of eigenvalues. Matrices $Z_{1},Z_{2}$ are not diagonal matrices because U does not always diagonalises them. But, as U is the eigenvector matrix of the linear combination of $K_{1}$ and $K_{2}$, the off-diagonal elements of $Z_{1}$ and $Z_{2}$ cancel each other out \citep{thomaz2004covariance, carlos05maximum}. Therefore,
\begin{eqnarray}
\Lambda  &=& diag[\mu_{1}\lambda^{1}_{1},\mu_{1}\lambda^{1}_{2},\dots,\mu_{1}\lambda^{1}_{n}]+ diag[\mu_{2}\lambda^{2}_{1},\mu_{2}\lambda^{2}_{2},\dots,\mu_{2}\lambda^{2}_{n}] \label{eigv_split_2} \\
&=& diag[\mu_{1}\lambda^{1}_{1}+\mu_{2}\lambda^{2}_{1}, \mu_{1}\lambda^{1}_{2}+\mu_{2}\lambda^{2}_{2},\dots,\mu_{1}\lambda^{1}_{n}+\mu_{2}\lambda^{2}_{n}]  \\ 
%\lvert \Lambda \rvert &=& \prod_{i=1}^{n}{(\mu_{1}\lambda^{1}_{i}+\mu_{2}\lambda^{2}_{i})} \\
%ln \lvert \Lambda \rvert &=& \sum_{i=1}^{n}ln{(\mu_{1}\lambda^{1}_{i}+\mu_{2}\lambda^{2}_{i})}
\end{eqnarray}
where $\mu_{1}\lambda^{1}_{1},\mu_{1}\lambda^{1}_{2},\dots,\mu_{1}\lambda^{1}_{n}$ are the variance of $K_{1}$ spanned by the U eigenvector matrix and $\mu_{2}\lambda^{2}_{1},\mu_{2}\lambda^{2}_{2},\dots,\mu_{2}\lambda^{2}_{n}$ are the variance of $K_{2}$. In order to maximize the Eqn-\eqref{max_det}, we need to maximize the individual eigenvalues of the combined covariance matrix. So, effectively it implies that we need to maximize each of the $(\mu_{1}\lambda^{1}_{i}+\mu_{2}\lambda^{2}_{i})$ terms. As stated previously, the eigenvalues of positive semi-definite matrices are non-negative. We have used kernel functions to compute similarities, which resulted in our similarity matrices being positive semi-definite. Since this is a convex combination of two terms, and all the eigenvalues are non-negative, therefore, in order to maximize it, we just need to take the maximum out of both the terms because,
\begin{eqnarray}
(\mu_{1}\lambda^{1}_{i}+\mu_{2}\lambda^{2}_{i}) \leq \max (\lambda^{1}_{i},\lambda^{2}_{i})
\end{eqnarray} 
when both $\mu_{i},\lambda_{i}$ are positive. Therefore, the maximum entropy is obtained at either ($\mu_{1}$=0, $\mu_{2}=1)$ or ($\mu_{1}$=1, $\mu_{2}=0)$ for each eigenvalue, i.e., we do not take the combination of both the terms but only one of them which is the maximum. To summarise, our matrices are not combined using any particular values of $\mu_{1},\mu_{2}$ but by just picking the maximum of both the variances spanned by U. 

Till now we discussed the theoretical justification of the technique, next we discuss the practical aspects of its implementation.

\subsection{Algorithm}
From the discussion of the preceding section it is clear that in order to calculate $U$, we need a $K$ which is an unbiased (a=b) linear combination of two similarity matrices, i.e., has equal contribution from both the matrices. Since any unbiased combination gives the same set of eigenvectors we have chosen $a=b=1$. The final algorithm is described in Algorithm-\ref{alg:max_ent_integration}.

\begin{algorithm}[h]
\caption{Maximum Entropy Similarity matrix Integration}
\label{alg:max_ent_integration}
\begin{algorithmic}[1]
\REQUIRE Similarity Matrices ($K_{1}$ and $K_{2}$) 

\STATE Calculate the eigenvectors $U$ of matrix $K$ obtained by $K = K_{1} + K_{2}$.

\STATE Use this $U$ to calculate the variance contribution of both $K_{1}$ and $K_{2}$. These are  
\begin{eqnarray}
	diag[UK_{1}U^{T}] &=&  diag[\lambda^{1}_{1},\lambda^{1}_{2},\dots,\lambda^{1}_{n}] \label{var_contrib1} \\
	diag[UK_{2}U^{T}] &=&  diag[\lambda^{2}_{1},\lambda^{2}_{2},\dots,\lambda^{2}_{n}] \label{var_contrib2}
\end{eqnarray}

\STATE Now form the final eigenvalue matrix $Z$ by choosing the maximum eigenvalues from each diagonal matrix (\ref{var_contrib1} and \ref{var_contrib2}).
\begin{displaymath}
	Z =  diag[max(\lambda^{1}_{1},\lambda^{2}_{1}),max(\lambda^{1}_{2},\lambda^{2}_{2}),\dots,max(\lambda^{1}_{n},\lambda^{2}_{n})] 
\end{displaymath}
\STATE Finally, compute the maximum entropy matrix 
\begin{displaymath}
	K^{ME} = UZU^{T}
\end{displaymath}
\end{algorithmic}
\end{algorithm}  

The principal idea here is that we keep the dominant eigenvalues, while getting rid of the smaller, and hence unreliable ones, and replacing it with better ones from the other dataset. 
\section{Datasets and Methodology} \label{chap3:sec:materials}

We have used the same datasets that was used in the previous chapter as discussed in Section-\ref{chap2:sec:materials}. They are the yeast microarray 
datasets \citep{gasch00genomicexpn,spellman98comprehensive}, DNA-binding dataset \citep{harbison04transcriptional}, 
PPI dataset (from MIPS Comprehensive Yeast Genome Database (CYGD)) \citep{Gueldener2006MPact} and the TF-gene interactions (YEASTRACT) \citep{Teixeira06yeastract}. 

While in the previous chapter, we used full set of genes from the microarray datasets and applied available constraints on them, in this chapter we are unable to use 
the full set of genes. This is because now we are combining two matrices which must be similar in size. Therefore, We first pre-process and find common genes between 
pairs of datasets that are being combined. In the previous chapter, after removing the NORFs, the stress dataset had 6251 genes and cell-cycle dataset had 6257 genes. 
They had 156 and 60 experiment counts respectively. The counts get changed after finding common sets of genes among dataset apirs. The final number of common genes when pairing 
microarray datasets with ChIP-chip, PPI and Yeastract datasets are   
 
\begin{table}
\centering
\begin{tabular}{|c|c|}
\hline
Dataset Pair & Number of genes \\ 
\hline
Stress and ChIP-chip & 2346 \\
Stress and PPI       & 4508 \\
Stress and Yeastract  & 5654 \\
\hline 
Cell-cycle and ChIP-chip & 2346\\
Cell-cycle and PPI       & 4508\\
Cell-cycle and Yeastract  & 5654\\
\hline
\end{tabular}
\caption{Number of genes considered for individual dataset pairs}
\label{tab:no_constraints}
\end{table}

After pre-processing, for the microarray datasets, we compute the similarity matrices from both of them using parameters obtained by 
the optimization procedure in the previous chapter. For the PPI, Yeastract and ChIP-chip datasets that are in the form 
of pairwise interactions, we have used the total within-cluster sum of square distances as we did not have access to original data vectors but only 
the similarity (or adjacency graph). For these datasets, we need to do the optimization of the parameter for computing the diffusion matrix. 
As shown in Equation-\ref{diffusion} we need to find the optimum value for $\beta$. To do this, we first compute diffused matrices for a range of $\beta$ values 
and then we do spectral clustering on each of these diffused matrices to find the best parameter, which is the one that yields the best cluster quality. This 
optimization is different from the microarray dataset ones because here we don't have the original data vectors because of the nature of graphical data where only 
links are specified. Because of this, we can't use distance based optimization techniques. So we have used a simpler and straightforward metric called \textit{withinss} in order 
to judge the cluster quality. For a clustering run of the algorithm, we compute this by finding the cumulative sum across all the clusters of sum of squared 
distance of all points in a cluster from its cluster centre.
\begin{equation}
    \mathit{withinss}=\sum_{i=1}^{k}\sum_{x_{ij}\in \mathbf{C}_{i}}(x_{ij}-\bar{x}_{i})^2
\end{equation}
where $C_{1},\dots,C_{k}$ are the different clusters and $\bar{x}_{i}$ are the cluster centres. Since we do not have access to the data points, we compute the \textit{withinss} of the vectors on which k-means clustering is done for spectral clustering (refer Step-6 of Algorithm-\ref{alg:spectral_clustering}). A better clustering will have a lower $withinss$ value (hence more compact). 
 
Once we have the similarity matrices from both datasets that are being integrated, we merge both of them using the maximum entropy technique as 
discussed in Algorithm-\ref{alg:max_ent_integration}. We then use spectral clustering on resulting similarity matrix to get our final clusters and then validate 
the biological significance of our results using the Gene Ontology annotations like the previous chapter.

\subsection{Parameter optimisation results}\label{param_optimisation}

\subsubsection{Chip-Chip, PPI and Yeastract datasets}
As seen in Figure-\ref{fig:chip_withinss}, for the ChIP-chip dataset, the optimum value (smallest vwithinss value implies tightest clustering) is at $\beta=10$ where the total 
withinss falls significantly. It also has one of the smallest standard deviation at that point. For the PPI dataset, as seen in Figure-\ref{fig:ppi_withinss}, the optimum 
value again is at $\beta=10$ where it has the smallest withinss value. Even though the standard deviation is not the lowest, yet we choose is because of the smallest value.
Figure-\ref{fig:yt_withinss} for the Yeastract dataset, has a optimum range between $\beta=5$ and $\beta=10$, where the optimum value is at $\beta=5$ while the combination of small value and small standard deviation 
is at $\beta=10$. We chose $\beta=5$ because of the smallest withinss value.    

\begin{figure}[htp]
  \begin{center}
    \subfigure[ChIP-chip $\beta$ optimization using total within-cluster sum of square distances]
            {\label{fig:chip_withinss}\includegraphics[scale=0.7]{/home/alok/phd/post_viva/maxent/results/sigmaopt/plots/chip_withinss.eps}}
    \subfigure[PPI $\beta$ optimization using total within-cluster sum of square distances]
            {\label{fig:ppi_withinss}\includegraphics[scale=0.7]{/home/alok/phd/post_viva/maxent/results/sigmaopt/plots/ppi_withinss.eps}} \\
    \subfigure[Yeastract $\beta$ optimization using total within-cluster sum of square distances]
            {\label{fig:yt_withinss}\includegraphics[scale=0.7]{/home/alok/phd/post_viva/maxent/results/sigmaopt/plots/yt_withinss.eps}}
  \end{center}
  \caption{ChIP-chip, PPI and Yeastract datasets: Beta optimization}
  \label{fig:chip_ppi_yt_opt}
\end{figure}

\section{Biological Significance Analysis}

\subsection{Numerical Biological Significance comparison (using mutual information)} \label{num_biosig_mi}
\subsubsection{Results}

\begin{table}[p]
\centering
{\footnotesize
\begin{tabular}{@{\extracolsep{\fill}}|p{0.5in}|p{0.40in}|p{0.50in}|p{0.40in}|p{0.40in}|p{0.40in}||p{0.40in}|p{0.50in}|p{0.40in}|p{0.40in}|p{0.40in}|}
\hline

Datasets & & & \multicolumn{3}{|l|}{Semi-supervised integration} & \multicolumn{3}{|l|}{Maximum entropy integration}\\ \cline{2-9}
       & Dataset A &  Dataset B & After integration & Gain for A & Gain for B & After Integration & \% Gain for A & \% Gain for B\\
\hline
Stress \& ChIP-chip &  62.3   & 27.5  & 59.6  &  -4.3\%& 116.72\%   & 59.4   & -4.65\%  &  116\% \\ \hline
Stress \& PPI       &  62.3   & 29.2  & 66.5  &   6.74\%&  127.73\%  & 75.1   &  20.54\% &  157.19\% \\ \hline
Stress \& Yeastract &  62.3   & 38.0  & 94    &  50.88\%&  147.36\%  & 109.0  &  74.95\% &  186.84\% \\ \hline

\end{tabular}
}
\caption{Stress microarray dataset: Comparison of Biological significance index before and after maximum entropy data integration}
\label{tab:stress:maxent_biol_index}
\end{table}

\begin{table}[p]
\centering
{\footnotesize
\begin{tabular}{@{\extracolsep{\fill}}|p{0.5in}|p{0.40in}|p{0.50in}|p{0.40in}|p{0.40in}|p{0.40in}||p{0.40in}|p{0.50in}|p{0.40in}|p{0.40in}|p{0.40in}|}
\hline

Datasets & & & \multicolumn{3}{|l|}{Semi-supervised integration} & \multicolumn{3}{|l|}{Maximum entropy integration}\\ \cline{2-9}
       & Dataset A &  Dataset B & After integration & Gain for A & Gain for B & After Integration & Gain for A & Gain for B\\
\hline
Cell-cycle \& ChIP-chip &  39.1   & 27.5  & 39.7  &   1.53\%     &  44.36\%  & 48.2   &  23.27\% &  75.27\% \\ \hline
Cell-cycle \& PPI       &  39.1   & 29.2  & 40.9  &   4.60\%     &  40.06\%  & 39.3   &  0.51\%  &  34.58\% \\ \hline
Cell-cycle \& Yeastract &  39.1   & 38.0  & 59.3  &   51.66\%    &  35.94\%  & 47.6   & 21.73 \% &  25.26\% \\ \hline

\end{tabular}
}
\caption{Cell-cycle microarray dataset: Comparison of Biological significance index before and after maximum entropy data integration}
\label{tab:ccycle:maxent_biol_index}
\end{table}

We follow the same procedure that was followed in the previous chapter (refer Section-\ref{num_biosig_mi}) for biological validation of the resulting clusters using the technique based on 
mutual information by \citet{Gibons2002Judging}. The results of the integration of the stress microarray dataset with ChIP-chip, PPI and Yeastract are in 
Table-\ref{tab:stress:maxent_biol_index}. We have reported percentage gains in index values before and after integration for each pair of dataset being integrated. In order to compare our 
results with the semi-supervised technique, we also reran the semi-supervised experiments on the filtered (common) gene sets. For each pair of dataset, the first one is referred 
to as A and the second as B. For example, in the first full row of Table-\ref{tab:stress:maxent_biol_index}, stress dataset is referred to as A, while Chip-Chip dataset is called B. 

The results in Table-\ref{tab:stress:maxent_biol_index} clearly indicate that both our data integration techniques helps improve the quality of resulting clustering biologically. 
Both datasets A and B have improved biological significance after integration except in one scenario where Chip-Chip is integrated with Stress dataset. MaxEnt 
integration has resulted in improvements for the stress datasets with every other dataset except the Chip-Chip dataset. We also observe that the MaxEnt technique has outperformed 
the semi-supervised one except the Chip-Chip dataset combination where the semi-supervised technique shows a small improvement than the MaxEnt. 
Also, more than the stress datasets, the improvement in B datasets i.e. the Chip-Chip, PPI and Yeastract datasets, when combined, is better. 

For the cell-cycle dataset, as shown in Table-\ref{tab:ccycle:maxent_biol_index}, the results are somewhat different. Here, too both the datasets, A and B, have improved biological 
significance after integration. MaxEnt technique has resulted in improved biological significance
after combination in all the cases. However, the results for MaxEnt are not better than the semi-supervised one. While for stress dataset, the Chip-Chip dataset had performed 
better than the MaxEnt one, for the cell-cycle dataset, it's the opposite. MaxEnt is better than the semi-supervised one for the Chip-Chip dataset combination. 
For PPI and Yeastract combinations, the semi-supervised is better.
 
As we discussed earlier that this technique merges two datasets by taking their dominant eigenvectors. In the case of stress dataset, both the datasets gained biological significance whereas now cell-cycle is always the loser. The PPI, Yeastract 
are both curated datasets. Most of the curated datasets are taken from experiments that are conducted in non-stress environments to study the regular activities of genes. 
Therefore, they are more similar to the cell-cycle dataset. Because of this similarity, the cell-cycle dataset has not gained much from the others. 
In case of stress, they were very dissimilar and hence both gained information from each other which led to improvement in their scores.    

\subsection{Qualitative Biological Significance (using Gene Ontology annotations)} \label{semisup_biosig_go}

We put the final clusters of genes (before and after combination) through Genomica which is a tool to analyze the characteristics of resulting clustering using Gene Ontology and has been detailed in the 
previous chapter.
 
\begin{figure}[p]
\centering
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.5]{/home/alok/phd/post_viva/maxent/results/analysis/img_maxent_stress_chip/1.eps}
\label{fig:stress_chip_enrich_1}
}
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.4]{/home/alok/phd/post_viva/maxent/results/analysis/img_maxent_stress_chip/2.eps}
\label{fig:stress_chip_enrich_2}
}
\label{fig:stress_chip_enrich}
\caption{Sections of the image showing significant enrichment in Stress dataset combined with Chip-Chip dataset. Full image available in the Appendix}
\end{figure}

\begin{figure}[p]
\centering
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.5]{/home/alok/phd/post_viva/maxent/results/analysis/img_maxent_stress_ppi/1.eps}
\label{fig:stress_ppi_enrich_1}
}
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.4]{/home/alok/phd/post_viva/maxent/results/analysis/img_maxent_stress_ppi/2.eps}
\label{fig:stress_ppi_enrich_2}
}
\label{fig:stress_ppi_enrich}
\caption{Sections of the image showing significant enrichment in Stress dataset combined with PPI dataset. Full image available in the Appendix}
\end{figure}

\begin{figure}[p]
\centering
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.5]{/home/alok/phd/post_viva/maxent/results/analysis/img_maxent_stress_yt/1.eps}
\label{fig:stress_ppi_enrich_1}
}
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.4]{/home/alok/phd/post_viva/maxent/results/analysis/img_maxent_stress_yt/2.eps}
\label{fig:stress_ppi_enrich_2}
}
\label{fig:stress_ppi_enrich}
\caption{Sections of the image showing significant enrichment in Stress dataset combined with Yeastract dataset. Full image available in the Appendix}
\end{figure}

\subsubsection{Stress vs Stress and Chip-Chip}
Nucleic acid binding, ribosome biogenesis  and assembly, macromolecule biosynthetic process, translation and biosynthetic process are the 
few common significant enrichment observed in both the datasets. Processes like mitotic cell cycle, regulation of cell cycle, nucleotide biosynthetic process, 
ribonucleotide metabolic process, oxidoreductase activity, response to pheromone, conjugation, cell division, reproduction, organic metabolic process, 
amine metabolic process, and amine biosynthetic process were the significant enrichment sites observed in Stress Chip Chip dataset as compared to Stress.

\subsubsection{Stress vs Stress and PPI}
Protein RNA complex assembly, macromolecule complex assembly, nucleic acid binding, DNA metabolic process, rRNA processing, RNA processing, 
ribosome biogenesis and assembly and RNA metabolic process are the common enrichment. Whereas, oxidoreductase activity, carbohydrate metabolic process, 
transcription from RNA polymerase II promoter, transcription regulator activity, response to stimulus, response to stress, protein binding, mitotic cell 
cycle, cell cycle, M phase, cell cycle phase, developmental process, cell development, chromosome organization and biogenesis, biological regulation, 
regulation of cellular process and transcription are processes which were observed as highly significant enrichment in the Stress PPI dataset.

\subsubsection{Stress vs Stress and Yeastract}
Macromolecule biosynthetic process, translation. R RNA processing, RNA processing, RNA metabolic process, ribosome biogenesis and assembly, 
protein RNA complex assembly, ribosome  assembly, 35S primary transcript processing, nucleic acid binding, RNA binding, processing of 20S pre-rRNA and 
DNA metabolic process are the common enriched zones in both the sets. Processes like cellular localization, transcription, cell development, macromolecule complex 
assembly, ribosomal large subunit biogenesis and assembly, ribosomal large subunit aseembly and maintenance, sno RNA binding, carbohydrate 
metabolic process, chromosome organization and biogenesis, DNA packaging, regulation of cellular process, biological regulation, response to stimulus, 
response to stress, developmental process, amine metabolic process, generation of precursor metabolites and energy, telomere maintenance, 
oxidoreductase activity and organic acid metabolic process  are the many significant enrichment observed only in the Stress YT dataset when 
compared to Stress dataset.

\begin{figure}[p]
\centering
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.5]{/home/alok/phd/post_viva/ec2machine/maxent/results/analysis/img_maxent_ccycle_chip/1.eps}
\label{fig:ccycle_chip_enrich_1}
}
\label{fig:ccycle_chip_enrich}
\caption{Sections of the image showing significant enrichment in Cell-cycle dataset combined with Chip-Chip dataset. Full image available in the Appendix}
\end{figure}

\begin{figure}[p]
\centering
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.5]{/home/alok/phd/post_viva/ec2machine/maxent/results/analysis/img_maxent_ccycle_ppi/1.eps}
\label{fig:ccycle_ppi_enrich_1}
}
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.5]{/home/alok/phd/post_viva/ec2machine/maxent/results/analysis/img_maxent_ccycle_ppi/2.eps}
\label{fig:ccycle_ppi_enrich_2}
}
\label{fig:ccycle_ppi_enrich}
\caption{Sections of the image showing significant enrichment in Cell-cycle dataset combined with PPI dataset. Full image available in the Appendix}
\end{figure}

\begin{figure}[p]
\centering
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.5]{/home/alok/phd/post_viva/ec2machine/maxent/results/analysis/img_maxent_ccycle_yt/1.eps}
\label{fig:ccycle_yt_enrich_1}
}
\subfigure[Section of the image showing significant enrichment]{
\includegraphics[scale=0.5]{/home/alok/phd/post_viva/ec2machine/maxent/results/analysis/img_maxent_ccycle_yt/2.eps}
\label{fig:ccycle_yt_enrich_2}
}
\label{fig:ccycle_yt_enrich}
\caption{Sections of the image showing significant enrichment in Cell-cycle dataset combined with Yeastract dataset. Full image available in the Appendix}
\end{figure}



\subsubsection{Cell-Cycle vs Cell-Cycle and Chip-Chip}
Translation, macromolecule biosynthetic process and biosynthetic process are the only three common significant enrichment observed 
in both the datasets. Whereas, ribosome biogenesis and assembly, cell cycle, cell cycle phase, mitotic cell cycle, regulation of 
cell cycle, cell wall organization and biogenesis, cell wall division, reproduction, conjugation, response to pheromone, organic 
acid metabolic process, amine metabolic process and response to stimulus are the other significant enrichment 
sites observed in the Cell Cycle Chip Chip dataset.

\subsubsection{Cell-Cycle vs Cell-Cycle and PPI}
DNA metabolic process, macromolecule biosynthetic process, biosynthetic process, translation, RNA metabolic process, rRNA processing, 
RNA processing are few common enrichment in both the datasets. However, response to DNA damage stimulus, cell cycle, mitotic cell cycle, 
cell cycle phase, M phase, biological regulation, regulation of cellular process, developmental process, regulation of transcription, 
regulation of metabolic process, transcription, cellular localization, cell development, response to stimulus, response to 
stress and ribosome biogenesis assembly are processes which showed significant enrichment in the Cell Cycle PPI dataset.

\subsubsection{Cell-Cycle vs Cell-Cycle and Yeastract}
Macromolecule biosynthetic process, translation, biosynthetic process, RNA metabolic process and DNA metabolic process are common, 
whereas, response to stimulus, cell cycle, ribosome biogenesis and assembly and generation of precursor metabolites and energy are the other 
significantly enriched processes observed only in the Cell Cycle Yeastract dataset.

The goal of integration is to have more biologically relevant clusters from which hypotheses could be derived to be carried out and validated in wet labs. All our result data is available upon request. 

\section{Related Work and Discussion}

Combining evidence from multiple biological datasets is not a new phenomenon. Earlier efforts in this direction 
related to clustering were classified under co-clustering. In this technique, the datasets are combined by assigning equal 
or varying weights to each dataset. For distance based clustering techniques, individual distances derived from each datasets 
are merged in order to come up with a new one which is then used for clustering. \citet{Hanisch2002Coclustering} 
proposed a distance metric that combines information from expression data and biological networks and uses it for clustering 
genes. They define a graph distance function on a metabolic network derived from MIPS \citep{Gueldener2006MPact} 
and combine it with a correlation-based distance function for microarray gene expression measurements. They assigned 
equal weights to both the sources and then used the resulting distance for hierarchical clustering. They show that their technique was 
able to find biologically meaningful clusters. \citet{huang2006incorporating} developed a similar algorithm in which 
instead of combining the two information sources with equal weights, they used a \textit{shrinkage} approach with the 
genes belonging to the same functional classes assigned zero distance (maximal similarity) and the rest of the genes using the distance 
calculated from the microarray data. This is then used for K-medoids clustering on simulated data as well as real one for gene function 
prediction. \citet{bramier2007coclustering} proposed co-clustering based on a combined distance metric from microarray 
gene expression data and Gene Ontology terms for \ac{SOM}. Apart from distance based clustering, various researchers have combined datasets 
using model based clustering as well \citep{pan06incorporating}.

Kernel integration has been used in the field of supervised learning in order to combine kernels from different datasets. 
\citet{Holloway2006MachineLearning} used it for predicting the TF binding locations on DNA. They have used 18 different 
datasets - both sequence and non-sequence (expression, GO) and calculated kernels from them. The goal was to combine the kernels and then 
use the final combination in \ac{SVM} for classification. They have used each kernel individually and calculated the \textit{F statistic} 
(a widely used measure for performance of a classifier). In order to combine the kernels, these F values were used as weights 
for each of the kernel. One of the drawbacks is that the F-statistic does not take into account the relationship of the variables but 
only a macro view of the performace of the whole kernel encoding. They reported that combining all datasets resulted in 73\% coverage 
of known interactions. 

A more mathematically sound approach to kernel integration was developed by \citet{lanck04kerneldatafusion}. They have formulated the optimization 
problem as a convex optimization problem and then used \ac{SDP} to solve it. The technique is both statistically sound and computationally efficient. 
They used this technique in order to classify different classes of proteins (membrane vs ribosomal) using kernels derived from protein 
sequence, microarray expression and protein-protein interaction data. They have reported an improvement in the classification results when 
the kernels are combined as compared to individual kernels. The biggest drawback of such techniques is that the weights that are assigned to each 
dataset do not take into account the correlation between different variables across datasets and assigns one common weight for the 
whole dataset. Our technique tries to solve it by picking only the highest eigenvectors.

\citet{lewis06svm} did an investigation of weighted and unweighted kernel combination in classifying GO terms associated with protein 
sequences using SVM. They came to the interesting conclusion that for this particular task, unweighted combination of kernels 
is better than weighted ones. In order to compute the weights they too have used the SDP technique. \citet{Liang2008Adaptive} 
developed a technique to learn an optimal diffusion kernel as a convex combination of many kernels constructed from biological networks 
and then used the optimal kernel for protein function prediction. They report superior performance for the combined kernel 
in comparison to individual kernels. 

Most of the above kernel combination techniques are based on \textit{supervised learning} and the individual kernel weights are 
optimized using some training data. But Kernel integration in an unsupervised setting (when training data is unavailable) is hard 
because we have no way to compute the individual weights. 

\citet{Tsuda2004Learning} used a very different approach to guess a kernel matrix from incomplete data. The underlying approach is that some 
values in the matrix are known and they try to fill in the rest so that the resulting matrix has maximum possible entropy. 
They used this to compute kernels from PPI data and metabolic network and used these for the classification task and report that 
the maximum entropy kernel beats the diffusion kernel in classification accuracy. \citet{fujibuchi2007classification} have used the maximum 
entropy devised by \citet{Tsuda2004Learning} and applied it on three microarray datasets (heterogeneous kidney carcinoma, noise introduced 
leukemia and hetergenous oral cavity carcinoma metastasis) for the purpose of evaluating its classification performance as compared to 
single kernels (linear, polynomial and rbf). They report better overall performance for the maximum entropy kernel compared 
to others in classification performance.  

In practise, the principle of maximum entropy is useful when applied to verifiable information. A piece of information is verifiable if it can 
be determined whether a given distribution is consistent with it. Given this information, the maximum entropy procedure consists 
of seeking the probability distribution which maximizes information entropy, subject to the constraints of the information. This constrained 
optimization problem is typically solved using the method of Lagrange multipliers. Both these approaches to kernel approximation 
\citep{Tsuda2004Learning, fujibuchi2007classification} using maximum entropy is based on maximising the entropy subject to certain 
constraints. However, this is very different from our approach where we do not treat it as a entropy maximisation subject to certain constraints. 
We try to assume a distribution with maximum entropy as the ideal distribution when no other information is available in order to combine the 
similarity matrices. So, when we have multiple sets of evidence, e.g. PPI and microarray data, we try to combine them through their similarity 
matrices such that the resulting distribution (of the combined similarity matrix) has the maximum entropy.

The ideal scenario of its use is where one of the datasets is a very noisy sample while the other one is a \textit{compendium} or 
reference dataset collected over time from various sources. The compendium acts as an average of all known observations, which when 
combined using this technique with the sample, fills in the eigenvalues about which the sample dataset is most \textit{confused} about. 
So, the sample dataset gets to keep all its prominent eigenvalues while borrowing ones from the compendium about which it is not so confident. 


\section{Conclusion}

We have proposed a technique to integrate two diverse datasets where one is available in the form of vectorial data while the 
other is available in the form of a graph. The core idea is based on work done by \citet{thomaz2004covariance}. While they had 
used it to combine covariance matrices where one of them is singular, we have extended its use to general kernel combination after 
observing the similarities between the properties of a covariance matrix and a similarity matrix obtained using a kernel function. 

Integrating such diverse datasets is not possible unless we resort to some kind of normalization. We have used similarity functions 
to compute similarity matrices for these datasets as the \textit{normalization} step. We have used the same techniques that we used 
in the previous chapter in order to compute the similarity function parameters.
 
While in the supervised setting we have an objective function that we can use to optimize the contribution of each of 
the similarity functions, in an unsupervised setting there is no such facility. Hence, most previous works have 
used ad-hoc techniques in order to integrate different datasets. We argue that under such a setting when no further 
evidence is available to assign weights to individual datasets then the \textit{principle of maximum entropy} 
is the most natural and valid choice. Apart from conceptual elegance, other benefits of this technique are 
that no time consuming optimization is required to search for contributions of each dataset and fairly simple and 
intuitive linear algebra computations are needed. Since our technique is quite generic, in future, our work 
can be extended to integrating other sources of data, for example  the similarity between the promoter sequences of genes.  

One of the key shortcomings for the biological validation of our results is that it is not possible to get datasets 
that were generated on the same strains under similar experimental conditions. Both our datasets were compiled by 
independent researchers. When datasets that are generated under similar conditions start becoming available we 
would be more confident in assessing the biological significance of the results.

There is no gold-standard for validating the clusters and there are no reference datasets on which we can compare 
the results with other techniques. Gene ontology while being one of the more informative sources of validation is 
still an indirect validation technique. Also, there is a fundamental limitation of the underlying experimental 
techniques, since microarrays themselves do not represent a single time point, but rather the integration of 
gene activity over a time period. 