In this chapter, we review the research done in data integration techniques for regulatory module discovery. Initial research in this area involved plain clustering of microarray data. This was followed by progressively sophisticated modelling as well as integration of various data types.
   
\section{Plain Clustering} 

When microarray data started becoming available in the late 1990s, a prime goal was to identify sets of genes that act together functionally to perform certain cellular tasks such as metabolism or cell-cycle functions. In this early phase of data analysis, various clustering algorithms, e.g. \citet{eisen1998cluster}, were applied in order to find such gene modules.  An assumption behind this clustering approach was that co-expression implied co-regulation. In other words, if sets of genes were showing similar patterns of microarray expression, they must be co-regulated and hence belong to the same module. So, co-expression was assumed to imply co-regulation and co-regulation was assumed to imply similar function. However, both these assumptions are not always correct. The validity of the resulting clusters could be tested by identifying common promoter elements on the upstream portion of genes within the same cluster on the assumption that genes are co-regulated because they have similar promoter elements. Another popular way to show validity was by using gene ontology to show that the majority of genes belonging to a module were similar in function. This was done by computing the enrichment of gene ontology terms in each of the clusters. Better clusters were expected to have more significant enrichment of these terms. In these early works, no external information was used to guide the process of clustering. A review of the early techniques based on ad-hoc as well as model based clustering can be found in \citet{jong02modeling}.

\section{Causal Networks} 
Naturally, the research community wanted to model the causal relationships among various genes in much more detail, and this precipitated a second phase of modelling in which mostly Bayesian networks and their variants, such as dynamic Bayesian networks (DBNs), were applied to model the gene regulatory processes \citep{friedman00using,husmeier03sensitivity,murphy99modelling,zou05dynamic}. \citet{friedman00using} were the first to utilise Bayesian networks for modelling gene expression data and they tried two types of local distribution - discrete (multinomial) and continuous (linear Gaussian) to express the relation between dependent genes. They tested the work on the microarray expression data of \citet{spellman98comprehensive}. When networks that modelled the data accurately were identified, two pairwise features were computed from them - Markov relations and order relations. The Markov relation just checks if each gene of a pair is in the Markov blanket of the other. This would imply a direct causal relationship between them indicating a biological relation. The order relation checks if X is ancestor of Y in all the networks of an equivalence class. This can be determined directly from the directed graph by checking whether there is a path from X to Y that is directed towards Y consistently. An order relation implies that the two genes have a role in some more complex regulatory process. Temporal aspects of data were incorporated into the model by adding a discrete variable as the root. They suggested that non-linear local and temporal models should be used for better accuracy. Their analysis of the results shows that the method is sensitive to the choice of local model and in the case of the multinomial distribution is also sensitive to the discretization method used. \citet{werhli06comparative} carried out a comparative study of the performance of modelling gene regulatory networks using \acp{GGM}, relevance networks and \acp{BN}. They used both laboratory data as well as simulated data to evaluate the different approaches. They observed that on both types of data, Bayesian networks outperformed both relevance networks and graphical Gaussian models. 

The major difficulty with this fine tuned modelling approach is that for such a high dimensional problem involving many thousands of genes, the amount of experimental data available is never enough for accurate modelling. Moreover, it is very hard to deal with the cyclical feedback nature of gene networks using Bayesian networks since, without the explicit incorporation of time, they only handle acyclic relationships among the variables. The end result of such models was that the performance was not good and not many verifiable findings were made \citep{husmeier03sensitivity}. In order to improve upon the results, work was done to incorporate better prior knowledge in the Bayesian network based modelling. \citet{imoto03combining} combined PPI, DNA binding, promoter element motifs as well as literature text mining. \citet{tamada03estimating,tamada05utilizing} also used similar diverse datasets to build Bayesian network models. 

\citet{ihmels02revealing} proposed an algorithm called \textit{Signature}, which performs bi-clustering, that is to say clustering genes, and conditions together based on expression data. It is unlike the more established bi-clustering algorithms in that it does not simultaneously generate data partitions but works in steps. The input to the algorithm is a set of genes and, in the first step, experimental conditions under which these genes change their expression above a threshold are chosen. In the second stage, all genes that have changed expression significantly under these conditions are selected. They evaluate the consistency of their clustering algorithm by analysing the recurrence of the output gene sets in their resulting modules when the input is mixed with irrelevant genes. The idea is that the results of any good algorithm should not deviate too much when slight perturbations are introduced in the data. A module is considered to be reliable if it is obtained from several distinct slightly perturbed input gene sets. Since it carries out a refinement of clusters in two stages, there can be no guarantee that the results would be clustered in a globally optimal manner. A better formulation might be to use the \ac{EM} algorithm in order to maximise their objective function. 
 

\section{Supervised Module Algorithms} 

After these initial frustrations in moving from very naive modelling (plain clustering) to highly detailed modelling (\acs{DBN}), research began to tread a path somewhere in the middle. This pragmatic approach did yield very good results and is still the basis of current research. One of the most complete studies using these types of weakly supervised methods was carried out by \citet{segal03module}. Their method, called \textit{Module Networks} algorithm, takes as input a gene expression data set and a large precompiled set of candidate regulatory genes and outputs groups of co-regulated genes (modules), their regulators, and a regulation program that specifies behaviours of the modules as a function of regulator's expression and the conditions under which regulation takes place. It uses an iterative procedure that searches for a regulation program for each module (set of genes) and is based on the \ac{EM} method that is initialised with the results of another clustering algorithm. For each cluster of genes, it searches for a regulation program that provides the best prediction of the expression profiles of genes in the module as a function of the expression of a small number of genes from the regulator set. After identifying regulation programs for all clusters, the algorithm re-assigns each gene to the cluster whose program best predicts its behaviour. It iterates till convergence, refining both the regulation program and the gene partition in each iteration. 

In their experiments, they compiled a set of regulators from the \ac{SGD} \citep{cherry98sgd} and the \ac{YPD} \citep{payne98ypd} based on annotations that broadly suggest that certain genes have a regulatory role, as either a transcription factor or a signalling protein. They also identified more potential regulators by finding genes similar to those above but removing the global regulators from the list. Microarray data for gene expression for yeast was collected from the \ac{SMD}. They chose a subset that had significant gene expression change and removed from this set the cluster known to be generic environmental response genes. Finally, they added all the genes from the regulator list above. With these two datasets (expression and regulators), they use a module network learning algorithm \citep{segal05learning} to find separate sets of regulators and the regulated modules. They obtained modules that showed significant similarity in promoter element motifs as well as annotations in the gene ontology compiled by the Gene Ontology \citet{GO}. 

At about this time, more significant prior knowledge started becoming available in the form of ChIP-chip DNA binding data and other sources as described in the previous chapter. The next step of research focused on ways of integrating these datasets in order to find gene modules. 

\citet{barjoseph03computational} describe an algorithm for discovering regulatory modules. Their algorithm is called \ac{GRAM}, and combines microarray expression data with DNA-binding data. This was one of the first papers to have combined these two sources in order to achieve better clusters. DNA-binding data provides direct physical evidence of regulation and thus offers an improvement on previous work where only indirect evidence of interaction, for example promoter sequences, were used for prior information. The GRAM algorithm begins by performing an exhaustive search over all possible combinations of transcription factors indicated by the DNA-binding dataset using certain (strict) threshold P-values. This yields sets of genes that are regulated by sets of transcription factors. This gene list is filtered by studying their expression patterns to find genes that show co-expression. These act as seeds for gene modules. The next pass revisits transcription factors and expands the seed modules by adding genes with a relaxed P-value criterion that show co-expression. GRAM allows a gene to be part of more than one module. They identified 106 modules with 655 distinct genes regulated by 68 transcription factors. Within a module, the role of each transcription factor was identified as activator or repressor by analysing the correlation between the transcription factor's expression and the expression of regulated genes. Validation was done by analysing the promoter gene sequences in same cluster using the TRANSFAC \citep{transfac1996} database to identify common sequences. 

\citet{AmosTanay04revealing} analysed several diverse datasets in an attempt to reveal the modular organisation of the yeast regulation system. They defined modules as groups of genes with statistically significant correlated behaviour across the diverse datasets. Their algorithm is called \ac{SAMBA} which is an extensible framework that can be easily updated when new datasets become available. In their analysis, they have integrated expression, PPI and DNA-binding datasets. In \ac{SAMBA}, all genomic information is modelled as weighted bi-partite graphs. Nodes on one side of graph represent genes while the other side represents properties of genes, for example proteins encoded by them. Edges between property nodes and gene nodes are assigned weights. A module is a sub graph of this bi-partite graph and a high quality module is defined as a heavy sub graph in the weighted bi-partite graph. The key point is that all sources of data are considered as properties of genes or proteins encoded by genes and there is one unified representation of all data as a bi-partite graph. Since their algorithm is based on combinatorial principles rather than graph theoretic (spectral) methods, there are no guarantees of a globally optimum partitioning. For evaluation, they found the biological significance of resulting clusters by calculating the enrichment score of all gene ontology (GO) terms associated with the genes of a module and later annotated the modules with the highest valued terms, that is to say those terms that are shared by the highest number of genes. They also analysed 600 base pairs in the upstream promoter region of the genes in a module for common motif enrichment. For each potential motif, they calculated the enrichment score among all the genes of the module. The positive aspect of their approach is that it utilises all sources of information in one uniform representation and only requires a measure of similarity of genes across a subset of properties. It also allows overlapping modules (with common genes), which is not a feature of traditional clustering algorithms. One of the limitations of their approach is that all sources of data are assigned equal weights and it isn't possible to weigh them separately according to reliability or importance. 

In a later piece of work, \citet{amos05integrative} extended the work described above by investigating the \ac{SAMBA} algorithm in more detail. They analysed more diverse datasets and focused more on the biological significance of the results, explaining them much more fully. The paper mainly describes a study of fresh data in the context of an extensive compendium of existing datasets using \ac{SAMBA}. They proposed that future work should be carried out on integration across species on the basis that transcription modules are highly conserved among species. 

The work of \citet{lemmens06Inferring} is similar to other module discovery algorithms in that they propose a very simple and intuitive algorithm to find co-regulated sets of genes that have similar expression profiles, the same binding transcription factors and a commonality of promoter motifs. The principal difference from other algorithms is that where others used motif information to validate their results, they have used it in order to find the modules itself. Their algorithm, known as ReMoDiscovery works in two passes. In the first pass, known as the \textit{seed discovery} step, tightly co-expressed genes having a minimum number of common transcription factors and a minimum number of common conserved motifs are put together in separate modules known as \textit{seed modules}. In the second pass, known as the \textit{seed extension} step, the size of the modules is increased by computing the mean of the module's gene expression and ranking the remainder of the genes in the dataset in order of their decreasing correlation with the mean profile. They compared their algorithm results with \ac{SAMBA} and \ac{GRAM} (discussed earlier) and reported their findings. All parameters, such as the cut off for various datasets, have been chosen without much justification, and the basic idea seems very similar to the work of \citet{barjoseph03computational}. Some of the comparison metrics used do not seem very sound, for example average functional enrichment values have been calculated for the modules without normalising to account for the size of the modules. Similarly, summary statistics like minimum and maximum number of genes in modules do not provide relevant information for comparison of algorithms . 

\citet{huang2006incorporating} investigated a traditional clustering method known as k-medoids which is a robust version of the k-means clustering method. Unlike k-means, which uses the mean of all genes in a cluster as its centre, k-medoids uses the most central gene (median). It is found by locating the one with minimum average dissimilarity to rest of the genes. They incorporated prior knowledge into it by modifying the distance metric used while clustering. They have used microarray expression data for clustering while biological knowledge about the known similarity between pairs of genes is derived from gene ontology. Previous approaches to include biological knowledge in distance based clustering methods have used gene ontology and metabolic pathways to estimate distance or similarity measures among gene pairs and then used these along with microarray expression based distance metrics to create an average distance, which is later used to cluster expression data. Huang and Pan used a \textit{shrinkage} approach for the distance metric to shrink it towards zero in cases where there is strong evidence that two genes are functionally related. Their algorithm has two steps in which the first step uses the shrunk distance metric to cluster genes whose functionality is known from gene ontology. The second step clusters the remaining genes. In the second step clustered genes are assigned to either one of the step one clusters or to a step two cluster, depending on their distance from the medoids. The shrinkage parameter is chosen using cross validation. They evaluated their algorithm using both simulated as well as real data. In a later piece of work, \citet{pan06incorporating} used known functions of genes from existing biological research to assign different prior probabilities for a gene to belong to a cluster. He developed an \ac{EM} algorithm for this stratified mixture model. 

The research described above concerns the evaluation of individual techniques to integrate data from multiple sources. Some researchers have also focused on creating generic frameworks for data integration. \citet{Troyanskaya2003Bayesian} developed a \textit{meta} framework for integration of diverse sources of data. We call it \textit{meta} because it doesn't directly integrate the datasets but uses results from other techniques like clustering algorithms and combines them with other evidence. Their proposed framework is known as MAGIC (Multisource Association of Genes by Integration of Clusters) and is based on a Bayesian network whose conditional probability tables have been built with the advice of yeast genetic experts. Given a pair of genes, it outputs the probability that they are functionally related after weighing the evidences from various sources. Evaluation of the predictions from the system is done using gene ontology data. 

Most of the techniques that we have described work well for real (numerical) data but are less effective when dealing with string data, for example gene sequences, or graph data such as protein interactions. In many cases ad-hoc techniques have been deployed. In an approach to this problem, \citet{lanck04statframework} have proposed a framework where such diverse data could be merged in a principled manner. It is based on kernel methods \citep{johnshaw2004kernelmethods} in which algorithms work on kernel matrices that are derived from pairwise similarity among variables using kernel functions. If a valid kernel function can be defined to encode the similarity between two variables, then the methods are applicable regardless of the different types of data - strings, vectorial or graphical - being used. This framework provides a means to integrate more diverse types of data as and when they become available in the future. The original paper proposed the framework only for supervised learning but extensions to unsupervised learning are possible. 

